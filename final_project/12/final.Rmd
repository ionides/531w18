---
title: "STATS 531 Final Project"
subtitle: "POMP Model Fitting for US Mass Shooting Data"
date: "4/25/2018"
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \usepackage{amsthm}
output:
  html_document:
    theme: flatly
    toc: yes
    number_sections: true
  pdf_document:
    keep_tex: yes
    toc: yes
csl: ecology.csl
---

\newcommand{\DEF}{\overset{\text{def}}{=}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\IND}{\mathbbm{1}}
\newcommand{\var}{\text{Var}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\logit}{\text{logit}}
\newcommand{\n}{\newline}
\newcommand\loglik{\ell}
\newcommand\R{\mathbb{R}}
\newcommand\data[1]{#1^*}
\newcommand\estimate[1]{\data{#1}}
\newcommand\params{\, ; \,}
\newcommand\transpose{\scriptsize{T}}
\newcommand\eqspace{\quad\quad\quad}
\newcommand\lik{\mathscr{L}}
\newcommand\profileloglik[1]{\ell^\mathrm{profile}_#1}
\newcommand\ar{\phi}
\newcommand\ma{\psi}

-----------

```{r knitr-opts,include=FALSE,purl=FALSE,cache=FALSE}
library(knitr)
prefix = 'final_project'
opts_chunk$set(
  progress=TRUE,
  prompt=FALSE,
  tidy=FALSE,
  highlight=TRUE,
  strip.white=TRUE,
  warning=FALSE,
  cache=TRUE,
  results='markup',
  fig.show='asis',
  size='small',
  fig.lp="fig:",
  fig.path=paste0("figure/",prefix,"-"),
  cache.path=paste0("cache/",prefix,"-"),
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  dpi=300,
  dev='png',
  dev.args=list(bg='transparent')
  )

```

# Introduction

Following the shooting at Marjory Stoneman Douglas High School in Parkland, Florida, and the subsequent activism of the students, there was a renewed interest in gun control policies in preventing mass shootings. We focus our efforts on modeling the *number of mass shooting fatalities* across years from 1982 to 2017. 

For a deeper understanding of what, exactly, is meant by the term *mass shooting*, we turn to Mother Jones' "A Guide to Mass Shootings in America" [1]. Before 2013, the FBI defined a mass shooting as a "single attack in a public place in which four or more victims were killed." In 2013, a mandate for a federal investigation into mass shootings lowered this threshold to three or more fatalities. The data set from Mother Jones' investigation into mass shootings [2] uses these thresholds.

Due to the importance of understanding such frequently occuring tragedies in the United States, we ask ourselves **whether or not we can use population and finance models to properly model US mass shooting fatalities**. Specifically, we fit three different partially observed Markov processes (POMP) models:

1. the Ricker model
2. a stochastic volatility model
3. GARCH(1,1)

First, we observe some simple exploratory analysis
```{r data_clean, echo=FALSE, include=FALSE}
set.seed(594709947L)
source('functions.R')

# Read data
dat = read.csv('data//mass_shootings.csv')

dat$Date = as.Date(dat$Date,format="%m/%d/%Y")

dat = dat[,c('Fatalities','Injured','Date')]

dat$Year = year(dat$Date)

dat$Month = month(dat$Date)

# Sum the fatalities per month
dat = sql("
select
  Year
  ,Month
  ,sum(Fatalities) as Fatalities
from dat
group by 1,2
having Year <> 2018
order by 1,2
")

#Fill in the missing month-years
dat0 = expand.grid(c(min(dat$Year):max(dat$Year)),
            c(1:12),0)
names(dat0) = c('Year', 'Month', 'Fatalities')

# Data at the year-month level
dat_month = sql("
select
  d0.Year as Year
  ,d0.Month as Month
  ,d0.Fatalities + d.Fatalities as Fatalities
from dat0 d0 left join dat d on d.Year=d0.Year and d.Month=d0.Month
order by 1,2
")

dat_month[is.na(dat_month$Fatalities),'Fatalities'] = 0

# Data at the year level
dat_year = sql("
select
  Year
  ,sum(Fatalities) as Fatalities
from dat_month
group by 1
")
```

```{r eda, echo=FALSE, fig.align="center"}
gg_time_plot = ggplot(dat_year, aes(Year, Fatalities)) + 
  geom_line() +
  ggtitle('US Mass Shooting Fatalities 1982 - 2017')
gg_time_plot = formatGG(gg_time_plot,title_size = 14)

gg_acf = autoplot(acf(dat_year$Fatalities, plot = FALSE))+
  ggtitle('AutoCorrelation Plot')
gg_acf = formatGG(gg_acf, title_size = 14)

dat_demeaned = diff(dat_year$Fatalities) - mean(diff(dat_year$Fatalities))
gg_time_plot_demeaned = ggplot(data.frame(Year = dat_year$Year[2:nrow(dat_year)], Fatalities = dat_demeaned), aes(Year, Fatalities)) + 
  geom_line() +
  ggtitle('US Mass Shooting Fatalities\n Differenced/Demeaned')
gg_time_plot_demeaned = formatGG(gg_time_plot_demeaned, title_size=14)

multiplot(gg_time_plot, gg_time_plot_demeaned, gg_acf)
```

We quickly notice that mass shooting fatalities were relatively uncommon until after 2010, when they spiked to 71 in 2016 and 117 in 2017. We also notice that the sample autocorrelation plot indicates that the fatalities are relatively uncorrelated across time-lags. Despite the relatively uncorrelated time-lags, we proceed with POMP analysis since we desire find usefulness in models that account for potential latent variables such as fatality density and fatality volatility.

# The Ricker Model
 
A common model in population biology, the Ricker model attempts to describe population growth and resource depletion. It models population growth as a function of population density, $P_n$ at a given time. In the context of mass shootings in America, it is sensible to assume some time-varying density in the number of fatalities that drives the total number of fatalities. Thus, we formally state the model:

\begin{align*}
Y_n | P_n &\sim \text{Poi}(\phi P_n)\\
P_{n+1} &= rP_n\exp(-P_n + \epsilon_n),\qquad \epsilon_n\sim N(0,\sigma^2).
\end{align*}

where $Y_n$ is the number of mass shooting fatalities in year $n$.

First, we create likelihood slices for the $\phi$ and $r$ parameters, and then plot the likelihood surface. The model implementation and figures are presented below.

```{r ricker_model, echo=TRUE}
# Create pomp object for year mass shootings

skel = "DN = r*N*exp(-N);" # add in determnistic skeleton

# Add stochastic Ricker model 
stochStep = "
e = rnorm(0,sigma);
N = r*N*exp(-N+e);
"

# Add in the (Poisson) measurement model  
ms_rmeasure = "Fatalities = rpois(phi*N);"
ms_dmeasure = "lik = dpois(Fatalities,phi*N, FALSE);"
ms_statenames = c("N","e")
ms_paramnames = c("phi","r","sigma")
ms_initializer = "
  N = 1;
  e = 6;
"

# Build pomp object
ms_pomp = pomp(
  data = dat_year,
  times = "Year",
  t0 = 1981,
  skeleton = map(Csnippet(skel)),
  rprocess = discrete.time.sim(
    step.fun=Csnippet(stochStep),
    delta.t=1
    ),
  rmeasure = Csnippet(ms_rmeasure),
  dmeasure = Csnippet(ms_dmeasure),
  statenames = ms_statenames,
  paramnames = ms_paramnames,
  initializer = Csnippet(ms_initializer),
  cdir=getwd()
  )
```

```{r likelihood_surface, fig.align="center", echo=FALSE, message=FALSE}
########################################################
# Construct likelihood slices for phi and r parameters
########################################################

sliceDesign(
  c(N.0=1,e.0=6,r=1.8,sigma=0.93,phi=35.3),
  phi=rep(seq(from=1,to=45,length=50),each=3),
  r=rep(seq(from=0.1,to=15,length=50),each=3)) -> p

set.seed(998468235L,kind="L'Ecuyer")
mcopts = list(preschedule=FALSE,set.seed=TRUE)

foreach (theta=iter(p,"row"),.combine=rbind,
         .inorder=FALSE,.options.multicore=mcopts) %dopar% 
 {
   pfilter(ms_pomp,params=unlist(theta),Np=5000) -> pf
   theta$loglik <- logLik(pf)
   theta
 } -> p


foreach (v=c("phi","r")) %do% 
{
  x <- subset(p,slice==v)
  plot(x[[v]],x$loglik,xlab=v,ylab="loglik")
}

expand.grid(phi=seq(from=5,to=50,length=50),
            r=seq(from=0,to=15,length=50),
            sigma=0.8,
            N.0 = 1,
            e.0 = 6) -> p

foreach (theta=iter(p,"row"),.combine=rbind,
         .inorder=FALSE,.options.multicore=mcopts) %dopar% 
 {
   pfilter(ms_pomp,params=unlist(theta),Np=5000) -> pf
   theta$loglik <- logLik(pf)
   theta
 } -> p

pp = mutate(p,loglik=ifelse(loglik>max(loglik)-100,loglik,NA))

ggplot(data=pp,mapping=aes(x=r,y=phi,z=loglik,fill=loglik))+
  geom_tile(color=NA)+
  geom_contour(color='black',binwidth=3)+
  scale_fill_gradient()+
  labs(x=expression(r),y=expression(phi))

```

We observe that $\hat{\phi} > 30$ and $\hat{r}\approx 2$ may potentially maximize the log-likelihood for our Ricker model. We use the IF2 algorithm [3] to carry out a global maximization of our log-likelihood using randomized starting values. This allows us to obtain parameter estimates and a log-likelihood.

```{r iterated_filtering, echo=F}
run_level = 2
switch(run_level,
       {ms_Np=100; ms_Nmif=10; ms_Neval=10; ms_Nglobal=10; ms_Nlocal=10}, 
       {ms_Np=2000; ms_Nmif=200; ms_Neval=10; ms_Nglobal=10; ms_Nlocal=10}, 
       {ms_Np=60000; ms_Nmif=300; ms_Neval=10; ms_Nglobal=100; ms_Nlocal=20}
)

# From looking at likelihood surface
ms_mle = c(phi = 40, r = 2.5, sigma = 0.8)

ms_rw.sd = 0.02
ms_cooling.fraction.50 = 0.5

######################################################################################
# Conduct LOCAL SEARCH of the likelihood surface using IF2 algorithm to maximize
# likelihood over parameter space
######################################################################################
stew(file=sprintf("local_search_ricker-%d.rda",run_level),{
  
  t_local <- system.time({
    mifs_local <- foreach(i=1:ms_Nlocal,.packages='pomp', .combine=c, .options.multicore=mcopts) %dopar%  {
      mif2(
        ms_pomp,
        start=ms_mle,
        Np=ms_Np,
        Nmif=ms_Nmif,
        cooling.type="geometric",
        cooling.fraction.50=ms_cooling.fraction.50,
        transform=TRUE,
        rw.sd=rw.sd(phi=ms_rw.sd, r=ms_rw.sd, sigma=ms_rw.sd)
      )
    }
  })
  
},seed=900242057,kind="L'Ecuyer")


################################################################################################
# Some parameter perturbations remain in last filtering iteration in mif2 call above, so inference
# is not entirely reliable. Thus, we evaluate the likelihood using replicated particle filters
# at each point estimate
################################################################################################

stew(file=sprintf("lik_local_ricker-%d.rda",run_level),{
    t_local_eval <- system.time({
    liks_local <- foreach(i=1:ms_Nlocal,.packages='pomp',.combine=rbind) %dopar% {
      evals <- replicate(ms_Neval, logLik(pfilter(ms_pomp,params=coef(mifs_local[[i]]),Np=ms_Np)))
      logmeanexp(evals, se=TRUE)
    }
  })
},seed=900242057,kind="L'Ecuyer")

results_local = data.frame(logLik=liks_local[,1],logLik_se=liks_local[,2],t(sapply(mifs_local,coef)))

######################################################################################
# Conduct GLOBAL SEARCH of the likelihood surface using IF2 algorithm to maximize
# likelihood over parameter space
######################################################################################

# One can specify a large box in parameter space that contains all parameter vectors
# which seem remotely sensible. If an estimation method gives stable conclusions with
# starting values drawn randomly from this box, this gives some confidence that an adequate
# global search has been carried out. 
ms_box = rbind(
  phi=c(30,40),
  r=c(1,4),
  sigma = c(.9,3)
)

# Carry out the likelihood maximizations from diverse starting points
# We reset only the starting parameters mifs_global[[1]] since the rest of the 
# call to mif2 can be read in from mifs_global[[1]]    
stew(file=sprintf("box_eval_ricker-%d.rda",run_level),{
  
  t_global <- system.time({
    mifs_global <- foreach(i=1:ms_Nglobal,.packages='pomp', .combine=c, .options.multicore=mcopts) %dopar%  
      mif2(
        mifs_local[[1]],
        start=c(apply(ms_box,1,function(x)runif(1,x[1],x[2]))),
        transform=TRUE
    )
  })
},seed=1270401374,kind="L'Ecuyer")

# Again, evaluate the likelihood and standard error
# by using replicated particle filters at each point estimate
stew(file=sprintf("lik_global_eval_ricker-%d.rda",run_level),{
  t_global_eval <- system.time({
    liks_global <- foreach(i=1:ms_Nglobal,.packages='pomp',.combine=rbind, .options.multicore=mcopts) %dopar% {
      evals <- replicate(ms_Neval, logLik(pfilter(ms_pomp,params=coef(mifs_global[[i]]),Np=ms_Np)))
      logmeanexp(evals, se=TRUE)
    }
  })
},seed=442141592,kind="L'Ecuyer")

plot(mifs_global)
```
```{r iterated_filtering_results, echo=T}
results_global <- data.frame(logLik=liks_global[,1],logLik_se=liks_global[,2],t(sapply(mifs_global,coef)))
summary(results_global$logLik,digits=5)
```

From the POMP diagnostics, we observe that the log-likelihood estimates, along with $\hat{\sigma}$ and $\hat{r}$ have converged to their maximum-likelihood estimates. However, we are less convinced that our estimate of $\phi$ has converged. Due to our flux allocation being over-burdened, our IF2 algorithm used 2,000 particles (`Np = 2000`) with 200 iterations (`Nmif = 200`). Further analysis would involve increasing the number of particles to potentially obtain a better estimate of $\hat{\phi}$. For our purposes, we will use $\hat{\phi} = 40$ from viewing the likelihood surface plotted above. The convergence plots indicate $\hat{r}\approx 1.8$ with $\hat{\sigma}\approx 0.93$, in addition to a log-likelihood of -147.7.

Below, we simulate from the model with our obtained MLE estimates while comparing it to our observed mass shootings data. We see that the Ricker model appears to capture the sharp and volatile peaks of our data. Thus, our data on mass shootings appears to be well-modeled by the Ricker model, with the assumption of a latent time-varying density of shootings.

```{r ricker_simulations, fig.align="center", echo=FALSE, message=FALSE}
# Simulate from whole POMP
coef(ms_pomp) = c(N.0=1,e.0=6,r=1.8,sigma=0.93,phi=35.3)

sims = simulate(ms_pomp,nsim=3,as.data.frame=TRUE,include.data=TRUE)

dat_sim = sims

dat_sim$Year = dat_year$Year

levels(dat_sim$sim) = c('Data', 'Simulation 1', 'Simulation 2', 'Simulation 3')
gg_sim = ggplot(data=dat_sim,mapping=aes(x=time,y=Fatalities))+
  geom_line()+
  ggtitle('Ricker Simulations')+
  ylab('Mass Shooting Fatalities')+
  xlab('Year')+
  facet_wrap(~sim)
formatGG(gg_sim)
```

# Stochastic Volatility Model

The second model we fit to our data is a stochastic volatility model from Breto (2014) [4], which models **leverage** at year $n$, which here will be the correlation between mass shooting fatalities for year $n-1$ and the increase in log volatility from year $n-1$ to $n$. Our primary motivation for fitting this model is the apparent volatility year-to-year in mass shooting fatalities; we hope that a successful model, such as this stohastic volatility model, can capture this. Thus, we create a POMP implementation of the following model:

\begin{align*}
Y_n &= \exp(H_n/2)\epsilon_n,\\
H_n &= \mu_h(1-\phi) + \phi H_{n-1} + \beta_{n-1}R_n\exp(-H_{n-1}/2) + \omega_n,\\
G_n &= G_{n-1} + \nu_n,
\end{align*}

where $\beta_n = Y_n\sigma_{\eta}\sqrt{1-\phi^2}$, $\{\epsilon_n\}$ is an iid $N(0,1)$ sequence, $\{\nu_n\}$ is an iid $N(0,\sigma^2_\nu)$ sequence, and $\{\omega_n\}$ is an iid $N(0,\sigma^2_\omega)$ sequence. Here, $H_n$ represents the log volatility of American mass shootings. This model is fit to the differenced and demeaned mass shootings data (plotted in the introduction). 

The implementation of this model is below.

```{r stoch_model, echo=TRUE}
ms_statenames = c("H","G","Y_state")
ms_rp_names = c("sigma_nu","mu_h","phi","sigma_eta")
ms_ivp_names = c("G_0","H_0")
ms_paramnames = c(ms_rp_names,ms_ivp_names)
ms_covarnames = "covaryt"

rproc1 = "
  double beta,omega,nu;
  omega = rnorm(0,sigma_eta * sqrt( 1- phi*phi ) * sqrt(1-tanh(G)*tanh(G)));
  nu = rnorm(0, sigma_nu);
  G += nu;
  beta = Y_state * sigma_eta * sqrt( 1- phi*phi );
  H = mu_h*(1 - phi) + phi*H + beta * tanh( G ) * exp(-H/2) + omega;
"
rproc2.sim = "
  Y_state = rnorm( 0,exp(H/2) );
 "

rproc2.filt = "
  Y_state = covaryt;
 "
ms_rproc.sim = paste(rproc1,rproc2.sim)
ms_rproc.filt = paste(rproc1,rproc2.filt)

ms_initializer = "
  G = G_0;
  H = H_0;
  Y_state = rnorm( 0,exp(H/2) );
"

ms_rmeasure = "
   y=Y_state;
"

ms_dmeasure = "
   lik=dnorm(y,0,exp(H/2),give_log);
"

# Transform parameters to be defined on whole real line

ms_toEstimationScale = "
  Tsigma_eta = log(sigma_eta);
  Tsigma_nu = log(sigma_nu);
  Tphi = logit(phi);
"

ms_fromEstimationScale = "
  Tsigma_eta = exp(sigma_eta);
  Tsigma_nu = exp(sigma_nu);
  Tphi = expit(phi);
"

# Build pomp model for filtering and parameter estimation
ms.filt = pomp(data=data.frame(y=dat_demeaned,
                     time=1:length(dat_demeaned)),
              statenames=ms_statenames,
              paramnames=ms_paramnames,
              covarnames=ms_covarnames,
              times="time",
              t0=0,
              covar=data.frame(covaryt=c(0,dat_demeaned),
                     time=0:length(dat_demeaned)),
              tcovar="time",
              rmeasure=Csnippet(ms_rmeasure),
              dmeasure=Csnippet(ms_dmeasure),
              rprocess=discrete.time.sim(step.fun=Csnippet(ms_rproc.filt),delta.t=1),
              initializer=Csnippet(ms_initializer),
              toEstimationScale=Csnippet(ms_toEstimationScale), 
              fromEstimationScale=Csnippet(ms_fromEstimationScale)
)
```

Again, we use the IF2 algorithm to carry out a global maximization of our log-likelihood using randomized starting values. Again, we use `Np = 2000` and `Nmif = 200` while noting that further analysis entails using more particles and iterations. The results and diagnostics are below.

```{r iterated_filtering_stoch, echo=F}
########################################################################################
# Simlate from model
########################################################################################
expit<-function(real){1/(1+exp(-real))}
logit<-function(p.arg){log(p.arg/(1-p.arg))}
params_test <- c(
     sigma_nu = exp(-4.5),  
     mu_h = 5,  	 
     phi = expit(3),	 
     sigma_eta = exp(-0.07),
     G_0 = 6,
     H_0=0
  )


sim1.sim <- pomp(ms.filt, 
               statenames=ms_statenames,
               paramnames=ms_paramnames,
               covarnames=ms_covarnames,
               rprocess=discrete.time.sim(step.fun=Csnippet(ms_rproc.sim),delta.t=1)
)


sim1.sim = simulate(sim1.sim,seed=1,params=params_test)

# Build filtering object by copying new simulated data into 
# the covariate slot, and put back apropriate version of rprocess
sim1.filt = pomp(sim1.sim, 
  covar=data.frame(
    covaryt=c(obs(sim1.sim),NA),
    time=c(timezero(sim1.sim),time(sim1.sim))),
  tcovar="time",
  statenames=ms_statenames,
  paramnames=ms_paramnames,
  covarnames=ms_covarnames,
  rprocess=discrete.time.sim(step.fun=Csnippet(ms_rproc.filt),delta.t=1)
)

run_level = 3
ms_Np =          c(100,1e3,5000)
ms_Nmif =        c(10, 100,200)
ms_Nreps_eval =  c(4,  10,  20)
ms_Nreps_local = c(10, 20, 20)
ms_Nreps_global =c(10, 20, 100)

###########################################################################
# Run iterated filtering on the mass shooting data with stochastic volatility model
###########################################################################

ms_rw.sd_rp = 0.02
ms_rw.sd_ivp = 0.1
ms_cooling.fraction.50 = 0.5

stew(file=sprintf("mif2_stochastic_vol-%d.rda",run_level),{
   t.if1 <- system.time({
   if1 <- foreach(i=1:ms_Nreps_local[run_level],
                  .packages='pomp', .combine=c,
                  .options.multicore=list(set.seed=TRUE)) %dopar% try(
                    mif2(ms.filt,
                         start=params_test,
                         Np=ms_Np[run_level],
                         Nmif=ms_Nmif[run_level],
                         cooling.type="geometric",
                         cooling.fraction.50=ms_cooling.fraction.50,
                         transform=TRUE,
                         rw.sd = rw.sd(
                            sigma_nu  = ms_rw.sd_rp,
                            mu_h      = ms_rw.sd_rp,
                            phi       = ms_rw.sd_rp,
                            sigma_eta = ms_rw.sd_rp,
                            G_0       = ivp(ms_rw.sd_ivp),
                            H_0       = ivp(ms_rw.sd_ivp)
                         )
                    )
                  )
    
    L.if1 <- foreach(i=1:ms_Nreps_local[run_level],.packages='pomp',
                      .combine=rbind,.options.multicore=list(set.seed=TRUE)) %dopar% 
                      {
                        logmeanexp(
                          replicate(ms_Nreps_eval[run_level],
                                    logLik(pfilter(ms.filt,params=coef(if1[[i]]),Np=ms_Np[run_level]))
                          ),
                          se=TRUE)
                      }
  })
},seed=318817883,kind="L'Ecuyer")

######################################################################################
# Conduct GLOBAL SEARCH of the likelihood surface using IF2 algorithm to maximize
# likelihood over parameter space
######################################################################################

ms_box = rbind(
 sigma_nu=c(.04,0.09),
 mu_h    =c(6,9),
 phi = c(0.1,0.4),
 sigma_eta = c(.05,.5),
 G_0 = c(8,10),
 H_0 = c(1,3)
)

stew(file=sprintf("box_eval_stochastic_vol-%d.rda",run_level),{
  t.box <- system.time({
    
    if.box <- foreach(i=1:ms_Nreps_global[run_level],.packages='pomp',.combine=c,
                  .options.multicore=list(set.seed=TRUE)) %dopar%
      
    mif2(
      if1[[1]],
      start=apply(ms_box,1,function(x)runif(1,x[1],x[2]))
      )
                  
    
    L.box <- foreach(i=1:ms_Nreps_global[run_level],.packages='pomp',.combine=rbind,
                      .options.multicore=list(set.seed=TRUE)) %dopar% {
                        set.seed(87932+i)
                        logmeanexp(
                          replicate(
                            ms_Nreps_eval[run_level],
                            logLik(pfilter(ms.filt,params=coef(if.box[[i]]),Np=ms_Np[run_level]))
                          ), 
                          se=TRUE)
                      }
  })
},seed=290860873,kind="L'Ecuyer")

# Look at diagnostic plots
plot(if.box)
```
```{r iterated_filtering_results_stoch, echo=T}
r.box = data.frame(logLik=L.box[,1],logLik_se=L.box[,2],t(sapply(if.box,coef)))
summary(r.box$logLik,digits=5)
```

We observe that the estimated log-likelihood appears to have converged, as have $\hat{\phi}$, $\hat{\sigma}_{\eta}$ and $\hat{\mu}_h$. However, due to our computational limitations with our flux allocation being overburdened, the remaining parameter estimates might be unstable. Futher analysis would entail assuring that these parameters converge. Regardless, we can be confident that we have effectively maximized the log-likelihood and obtained a reliable estimator. We estimate the log-likelihood to be -157.9. With six fitted parameters, we obtain an AIC value of `r round(2*6 - 2*median(r.box$logLik),1)`. We eventually compare this AIC value to a benchmark model, namely the GARCH(1,1) model. 

We simulate from this stochastic volatility model using our parameter estimates and compare with the differenced/demeaned data:

```{r stoch_sim, fig.align="center", echo=FALSE, message=FALSE}
sim1.sim <- pomp(ms.filt, 
               statenames=ms_statenames,
               paramnames=ms_paramnames,
               covarnames=ms_covarnames,
               rprocess=discrete.time.sim(step.fun=Csnippet(ms_rproc.sim),delta.t=1)
)

params_test <- c(
     sigma_nu = .06,  
     mu_h = 6.27,  	 
     phi = .257,	 
     sigma_eta = .035,
     G_0 = 9.76,
     H_0=1.1
  )

sim1.sim = simulate(sim1.sim,seed=1,params=params_test, as.data.frame = T, include.data = T, nsim = 3)

dat_sim = sim1.sim

dat_sim$Year = dat_year$Year[2:nrow(dat_year)]

levels(dat_sim$sim) = c('Data', 'Simulation 1', 'Simulation 2', 'Simulation 3')

gg_sim = ggplot(data=dat_sim,mapping=aes(x=Year,y=y))+
  geom_line()+
  ggtitle('Stochastic Volatility Simulations')+
  ylab('Mass Shooting Fatalities')+
  xlab('Year')+
  facet_wrap(~sim)
formatGG(gg_sim, title_size = 14)

```

We observe that the simulations clearly reflect the data we observed, lending credit to the idea that financial POMP models could be used successfully to describe problems in the United States such as mass shooting fatalities.

# GARCH(1,1)

We fit the GARCH(1,1) model to our mass shooting data as a standard of comparison for the stochastic volatility model above. The model is outlined below:

\begin{align*}
Y_n &= \epsilon_n\sqrt{V_n},\\
V_n &= \alpha_0 + \alpha_1Y^2_{n-1} + \beta_1V_{n-1},
\end{align*}
where $Y_n$ is the differenced and demeaned data and $\epsilon_{1:N}$ is a white noise process. After fitting the model with three parameters, we obtain the results below.


```{r garch, echo=T}
fit.garch.benchmark <- garch(dat_demeaned,grad = "numerical", trace = FALSE)
L.garch.benchmark <- logLik(fit.garch.benchmark)
L.garch.benchmark
AIC(L.garch.benchmark)
```

The GARCH(1,1) model gives a log-likelihood of `r round(L.garch.benchmark, 1)` with an AIC of `r round(AIC(L.garch.benchmark), 1)`. The AIC of this benchmark model is actually less than that of the more complicated stochastic volatility model, which gave an AIC of `r round(2*6 - 2*median(r.box$logLik),1)`. Since we prefer models with lower AIC values, we can suggest that the GARCH(1,1) model might actually have more predictive power.

# Conclusions

As noted in the introduction, we set out to determine if population and financial POMP models could acurately describe mass shooting fatalities in the United States. We have demonstrated that models like the Ricker model, stochastic volatility models, and the GARCH(1,1) model have applications far outside the domains in which they were created.

Simulations from the Ricker and stochastic volatility models with respective MLE estimates appear to replicate the data fairly well, while the GARCH(1,1) model may actually have *more* predictive power than the stochastic volatility model due to a lower AIC value.

As mentioned in the body of this report, future analysis includes ramping up computing power to obtain even better parameter estimates. In addition, a wider range of financial and population POMP models should be fit to the data to observe more fully the extent to which these models can be applied to mass shooting fatalities.

# References

[1] Follman, Mark. "A Guide to Mass Shootings in America," MotherJones, 20 July 2012, [www.motherjones.com/politics/2012/07/mass-shootings-map/](www.motherjones.com/politics/2012/07/mass-shootings-map/).

[2] Follman, Mark. "US Mass Shootings, 1982-2018: Data From Mother Jones' Investigation," MotherJones, December 2012, [www.motherjones.com/politics/2012/12/mass-shootings-mother-jones-full-data/](www.motherjones.com/politics/2012/12/mass-shootings-mother-jones-full-data/).

[3] Ionides, E. L., D. Nguyen, Y. Atchadé, S. Stoev, and A. A. King. 2015. Inference for dynamic and latent
variable models via iterated, perturbed Bayes maps. Proceedings of the National Academy of Sciences of
USA 112:719-724.

[4] Bretó, C. 2014. On idiosyncratic stochasticity of financial leverage effects. Statistics & Probability Letters 91:20-26.